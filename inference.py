import torch
import pandas as pd
import numpy as np
import os
from torch.utils.data import DataLoader
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler, LabelEncoder

from src.config import Config
from src.dataset import SoccerEventDataset
from src.model import SpatialTemporalTransformer
from src.utils import seed_everything
from src.feature_engineering import feature_engineering

# -----------------------------------------------------------
# [ë„ìš°ë¯¸ í•¨ìˆ˜] Test ë°ì´í„°ì— Trainì˜ ê¸°ì¤€(Encoder) ì ìš©í•˜ê¸°
# -----------------------------------------------------------
def apply_train_encoding(train_df, test_df, cat_cols):
    """
    Train ë°ì´í„°ë¡œ LabelEncoderë¥¼ í•™ìŠµ(fit)ì‹œí‚¤ê³ ,
    Test ë°ì´í„°ì— ê·¸ ê·œì¹™ì„ ì ìš©(transform)í•©ë‹ˆë‹¤.
    ìƒˆë¡œìš´ ì¹´í…Œê³ ë¦¬(Unknown)ëŠ” -1 ë˜ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    encoders = {}
    for col in cat_cols:
        le = LabelEncoder()
        # Trainì˜ ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•™ìŠµ (ì—ëŸ¬ ë°©ì§€)
        train_values = train_df[col].astype(str).unique()
        le.fit(train_values)
        encoders[col] = le
        
        # Test ë³€í™˜ (Unknown ì²˜ë¦¬ í¬í•¨)
        test_values = test_df[col].astype(str).values
        # le.transformì€ ëª¨ë¥´ëŠ” ê°’ì´ ì˜¤ë©´ ì—ëŸ¬ê°€ ë‚˜ë¯€ë¡œ map ë°©ì‹ì„ ì‚¬ìš©
        # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜í‘œ ìƒì„± {Class: Index}
        mapping = {cls: idx for idx, cls in enumerate(le.classes_)}
        
        # ë§¤í•‘ ì ìš© (ì—†ìœ¼ë©´ 0ë²ˆìœ¼ë¡œ ëŒ€ì²´ - ë³´í†µ 0ë²ˆì´ ê°€ì¥ í”í•œ í´ë˜ìŠ¤ê±°ë‚˜ ì„ì˜ ì§€ì •)
        # ë” ì •êµí•˜ê²Œ í•˜ë ¤ë©´ 'Unknown' í´ë˜ìŠ¤ë¥¼ ì¶”ê°€í•´ì•¼ í•˜ì§€ë§Œ ì—¬ê¸°ì„  0ìœ¼ë¡œ ì²˜ë¦¬
        test_df[col] = [mapping.get(val, 0) for val in test_values]
        
    return test_df, encoders

# -----------------------------------------------------------
# Main Inference Logic
# -----------------------------------------------------------
def find_actual_data_path(meta_df_path_sample, start_dir='.'):
    target_filename = os.path.basename(meta_df_path_sample)
    print(f"ğŸ” ë°ì´í„° ìœ„ì¹˜ ì°¾ëŠ” ì¤‘... ({target_filename})")
    for root, dirs, files in os.walk(start_dir):
        if target_filename in files:
            full_path = os.path.join(root, target_filename)
            dir_containing_file = os.path.dirname(full_path) 
            test_root = os.path.dirname(dir_containing_file)
            return test_root
    return None

def load_test_data(meta_path, seq_len):
    # Test ë°ì´í„° ë¡œë“œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
    try:
        meta_df = pd.read_csv(meta_path)
    except:
        print("ë©”íƒ€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
        return None, None

    first_path = meta_df.iloc[0]['path']
    real_test_root = find_actual_data_path(first_path)
    if real_test_root is None: return None, None
    
    print(f"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {real_test_root}")
    all_sequences = []
    episode_ids = [] 
    
    for idx, row in tqdm(meta_df.iterrows(), total=len(meta_df)):
        parts = row['path'].replace('\\', '/').split('/')
        relative_part = os.path.join(parts[-2], parts[-1])
        file_path = os.path.join(real_test_root, relative_part)
        try:
            df = pd.read_csv(file_path)
            if len(df) < seq_len:
                pad_len = seq_len - len(df)
                pad = pd.DataFrame([df.iloc[0]] * pad_len, columns=df.columns)
                df = pd.concat([pad, df], ignore_index=True)
            else:
                df = df.iloc[-seq_len:]
            
            df['game_id'] = row['game_episode']
            all_sequences.append(df)
            episode_ids.append(row['game_episode'])
        except: pass

    full_test_df = pd.concat(all_sequences, ignore_index=True)
    return full_test_df, episode_ids

def inference():
    seed_everything(Config.SEED)
    device = Config.DEVICE
    print(f"Inference Device: {device}")

    # 1. [ì¤‘ìš”] Train ë°ì´í„° ë¡œë“œ (ê¸°ì¤€ ì¡ê¸°ìš©)
    print("ğŸ“ í•™ìŠµ ë°ì´í„°(Train) ë¡œë“œ ì¤‘... (ê¸°ì¤€ì  ì„¤ì •ì„ ìœ„í•´ í•„ìš”)")
    train_df = pd.read_csv(Config.TRAIN_PATH)
    
    # Train í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (velocity ë“± ìƒì„±)
    # feature_engineering í•¨ìˆ˜ê°€ (df, encoders)ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
    # ì—¬ê¸°ì„œ ë°˜í™˜ë˜ëŠ” encodersëŠ” ë¬´ì‹œí•˜ê³ , ì•„ë˜ì—ì„œ ì•ˆì „í•˜ê²Œ ë‹¤ì‹œ ë§Œë“­ë‹ˆë‹¤.
    train_df, _ = feature_engineering(train_df) 
    train_df = train_df.fillna(0)

    # 2. Test ë°ì´í„° ë¡œë“œ
    test_df, episode_ids = load_test_data("./data/raw/test.csv", Config.SEQ_LEN)
    if test_df is None: return
    
    # Test í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    test_df, _ = feature_engineering(test_df)
    test_df = test_df.fillna(0)

    print(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ - Train: {train_df.shape}, Test: {test_df.shape}")

    # 3. [í•µì‹¬] Train ê¸°ì¤€ìœ¼ë¡œ ì¸ì½”ë”© & ìŠ¤ì¼€ì¼ë§ ì ìš©
    print("âš–ï¸ í•™ìŠµ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ ë° ì¸ì½”ë”© ì ìš© ì¤‘...")
    
    # (1) ë²”ì£¼í˜• ë³€ìˆ˜ (Label Encoding)
    cat_cols = ['type_name', 'team_id']
    test_df, _ = apply_train_encoding(train_df, test_df, cat_cols)
    
    # (2) ì—°ì†í˜• ë³€ìˆ˜ (StandardScaler)
    # Train ë°ì´í„°ë¡œ Scaler í•™ìŠµ
    cont_cols = ['start_x', 'start_y', 'time_diff', 'velocity', 'dist_to_goal', 'angle_to_goal']
    scaler = StandardScaler()
    scaler.fit(train_df[cont_cols].values) # Trainìœ¼ë¡œ Fit!
    
    # 4. Dataset ìƒì„± (ë§Œë“¤ì–´ì§„ scaler ì „ë‹¬)
    test_dataset = SoccerEventDataset(test_df, seq_len=Config.SEQ_LEN, is_train=False, scaler=scaler)
    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)
    
    # 5. ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
    num_cont_features = len(cont_cols)
    cat_dims = [26, 12] # í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ ê³ ì •
    
    model = SpatialTemporalTransformer(
        num_cont_features=num_cont_features, 
        cat_dims=cat_dims, 
        embed_dim=Config.EMBED_DIM,
        num_layers=Config.NUM_LAYERS,
        seq_len=Config.SEQ_LEN,
        nhead=4
    ).to(device)
    
    model_path = Config.MODEL_SAVE_PATH
    try:
        if device.type == 'cpu':
            model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
        else:
            model.load_state_dict(torch.load(model_path))
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    model.eval()
    all_predictions = []
    
    print("ğŸš€ ì¶”ë¡  ì‹œì‘...")
    with torch.no_grad():
        for x_cont, x_cat in tqdm(test_loader):
            x_cont = x_cont.to(device)
            x_cat = x_cat.to(device)
            outputs = model(x_cont, x_cat)
            all_predictions.append(outputs.cpu().numpy())
            
    predictions = np.concatenate(all_predictions, axis=0)
    predictions[:, 0] = np.clip(predictions[:, 0], 0, 105)
    predictions[:, 1] = np.clip(predictions[:, 1], 0, 68)
    
    # 6. ì œì¶œ íŒŒì¼ ìƒì„± (ID + ì˜ˆì¸¡ê°’)
    save_path = './submission.csv'
    if len(episode_ids) == len(predictions):
        submission = pd.DataFrame({
            'game_episode': episode_ids,
            'end_x': predictions[:, 0],
            'end_y': predictions[:, 1]
        })
        submission.to_csv(save_path, index=False)
        print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_path}")
    else:
        print(f"âš ï¸ ê°œìˆ˜ ë¶ˆì¼ì¹˜ (ID: {len(episode_ids)} vs Pred: {len(predictions)})")
        df_result = pd.DataFrame(predictions, columns=['end_x', 'end_y'])
        df_result.to_csv(save_path, index=False)
        print("ë¹„ìƒ ì €ì¥ ì™„ë£Œ")

if __name__ == '__main__':
    inference()